{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **Project One**\n",
    "Stop Words\n",
    "\n",
    "## **지속성장 보고서 불용어 추출**\n",
    "보고서간의 특징구별 단어들 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.pos(\"삼성전자 글로벌센터 전자사업부\", stem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **1 text 문서에서 token 추출하기**\n",
    "Document 에서 한글 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - pdf 에서 변환한 Document 불러오기\n",
    "filename = './data/kr-Report_2018.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    texts = f.read()\n",
    "texts[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - 한글만 추출\n",
    "import re\n",
    "texts     = texts.replace('\\n', ' ')   # 해당줄의 줄바꿈 내용 제거\n",
    "tokenizer = re.compile(r'[^ ㄱ-힣]+')   # 한글과 띄어쓰기를 제외한 모든 글자를 선택\n",
    "texts     = tokenizer.sub('', texts)   # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "texts[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Token으로 변환한다\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens    = word_tokenize(texts)\n",
    "tokens[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - 복합명사는 묶어서 Filtering 출력\n",
    "# ex) 삼성전자의 스마트폰은 -- > 삼성전자 스마트폰\n",
    "\n",
    "noun_token = []\n",
    "for token in tokens:\n",
    "    token_pos = twitter.pos(token)\n",
    "    temp      = [txt_tag[0]   for txt_tag in token_pos   \n",
    "                              if txt_tag[1] == 'Noun']\n",
    "    if len(\"\".join(temp)) > 1:\n",
    "        noun_token.append(\"\".join(temp))\n",
    "\n",
    "texts = \" \".join(noun_token)\n",
    "texts[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 StopWord 데이터 필터링**\n",
    "**stopwords.txt** : 2015, 2016, 2017, 2018년 모두 존재하는 단어목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.txt : 2015, 2016, 2017, 2018 모두 출현했던 단어들 불러오기\n",
    "with open('./data/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "\n",
    "stopwords = stopwords.split(' ')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링 텍스트를 살펴보기\n",
    "from nltk.tokenize import word_tokenize\n",
    "texts = word_tokenize(texts)\n",
    "texts[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords 를 활용하여 Token을 필터링\n",
    "texts = [text for text in texts  \n",
    "              if text not in stopwords]\n",
    "\n",
    "# pandas 를 활용하여 상위빈도 객체를 출력한다\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "freqtxt = pd.Series(dict(FreqDist(texts))).sort_values(ascending=False)\n",
    "freqtxt[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **3 Konlpy 의 단점들**\n",
    "오타/ 비정형 텍스트의 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.pos('가치창출')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.pos('갤러시')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **4 WordCloud 출력**\n",
    "visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud 출력\n",
    "from wordcloud import WordCloud\n",
    "wcloud = WordCloud('./data/D2Coding.ttf',\n",
    "                   relative_scaling = 0.2,\n",
    "                   background_color = 'white').generate(\" \".join(texts))\n",
    "wcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
