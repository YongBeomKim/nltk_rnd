{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **나이브베이즈 한글 적용**\n",
    "nltk를 활용한 네이버 영화리뷰 평가모델 [nltk book](https://www.nltk.org/book/ch06.html) | [nltk How to](http://www.nltk.org/howto/classify.html)\n",
    "1. 0 : 부정적인 리뷰\n",
    "1. 1 : 긍정적인 리뷰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **1 NAVER 영화리뷰 데이터 전처리**\n",
    "1. https://github.com/e9t/nsmc\n",
    "1. https://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cat ./data/ratings_test.txt | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]   # header 제외\n",
    "        \n",
    "    from random import randint\n",
    "    random_data = [data[randint(1, len(data))]  for no in range(int(len(data)/20)) ]\n",
    "    return random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data('./data/ratings_train.txt')\n",
    "test_data  = read_data('./data/ratings_test.txt')\n",
    "\n",
    "print('Train_data ({})\\nsample : {}\\nTest_data  ({})\\nsample : {}'.format(\n",
    "    len(train_data), train_data[:3],\n",
    "    len(test_data),  test_data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "\n",
    "def tokenize(doc):\n",
    "    result = ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n",
    "    return result \n",
    "\n",
    "train_docs = [(tokenize(row[1]), row[2])    for row in train_data]\n",
    "test_docs  = [(tokenize(row[1]), row[2])    for row in test_data]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(train_docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t   for d in train_docs \n",
    "              for t in d[0]]\n",
    "print(\"Token Total :{}\\nSample : {}\".format(\n",
    "    len(tokens), tokens[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 nltk 를 활용하여 연산모델 만들기**\n",
    "https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens, name='네이버리뷰')\n",
    "\n",
    "print(\"number of Token : {} \\nunique Token    : {}\\n\".format(\n",
    "    len(text.tokens), len(set(text.tokens))))\n",
    "pprint(text.vocab().most_common(20))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc('font', family=['NanumGothic','Malgun Gothic'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,5))\n",
    "text.plot(50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **3 모델의 정확도/ 일반화를 높이는 추가작업**\n",
    "우도 상위 4000개 데이터를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색가능 단어목록 추출하기\n",
    "selected_words = [f[0] for f in text.vocab().most_common(4000)]\n",
    "selected_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mission 1\n",
    "# selected 객체를 './data/selected.words' 로 저장하기\n",
    "import pickle\n",
    "# pickle.dump(selected_words, open('./data/selected.words', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def term_exists(doc):\n",
    "    return {'exists({})'.format(word): (word in set(doc)) for word in selected_words}\n",
    "\n",
    "train_xy   = [(term_exists(d), c) for d, c in train_docs]\n",
    "test_xy    = [(term_exists(d), c) for d, c in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장별 Text을 분석가능한 Token으로 변환\n",
    "line_num = 130\n",
    "print(\"100th 문장 변환된 Token 갯수 : {}\\n\\nSample : {}\".format(\n",
    "    len(train_xy[line_num][0]),\n",
    "    train_xy[line_num][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifiers = nltk.NaiveBayesClassifier.train(train_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mission 2\n",
    "# classifiers 객체를 './data/classifiers.model' 로 저장하기\n",
    "import pickle\n",
    "# pickle.dump(classifiers, open('./data/classifiers.model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **4 생성한 모델을 평가**\n",
    "Accuracy\n",
    "1. 0 : 부정리뷰\n",
    "1. 1 : 긍정리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "# 학습한 모델객체 저장하기\n",
    "# Google 에 찾아보기 : Python classifier Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test 데이터로 Model의 정확도 측정\n",
    "'네이버 영화리뷰 모델의 Accuracy : {}'.format(\n",
    "    nltk.classify.accuracy(classifiers, test_xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **5 모델의 활용**\n",
    "1. 0 : 부정리뷰\n",
    "1. 1 : 긍정리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"졸잼 굿 최고입니다 최고\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 데이터를 Tagged Token 생성하기\n",
    "review = tokenize(review)    \n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagged Token 중 selected_words 포함여부 판단\n",
    "review = term_exists(review) \n",
    "for k, v in review.items():\n",
    "    if v == True:\n",
    "        print(\"{} = {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = classifiers.classify(review)  # 분류모델 평가\n",
    "if result == '1': \n",
    "    print('긍정리뷰')\n",
    "else:             \n",
    "    print('부정리뷰')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
